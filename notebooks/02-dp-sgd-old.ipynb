{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tj6QO8Iz4ZMv",
        "outputId": "21812d01-2c74-4cc1-c6d3-da2158e1d90c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "from opacus import PrivacyEngine\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# =========================================\n",
        "# 1. LOAD DATA\n",
        "# =========================================\n",
        "\n",
        "df_raw = pd.read_csv(\"data/synthetic_telemetry_data.csv\")\n",
        "\n",
        "# Keep original time string for later human friendly output\n",
        "df = df_raw.copy()\n",
        "df = df.drop(columns=[\"User ID\"], errors=\"ignore\")\n",
        "\n",
        "# Create a numeric timestamp column for modeling\n",
        "df[\"TimeSeconds\"] = pd.to_datetime(df[\"Time of Event\"]).astype(\"int64\") // 1_000_000_000\n",
        "\n",
        "categorical_cols = [\"Product Type\", \"Event Type\"]\n",
        "numeric_cols = [\"TimeSeconds\"]\n",
        "\n",
        "# =========================================\n",
        "# 2. FIT COLUMN TRANSFORMER ON REAL DATA\n",
        "# =========================================\n",
        "\n",
        "transformer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(sparse_output=False), categorical_cols),\n",
        "        (\"num\", StandardScaler(), numeric_cols),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Fit on real data and keep the transformed matrix\n",
        "X_real = transformer.fit_transform(df[categorical_cols + numeric_cols])\n",
        "\n",
        "# =========================================\n",
        "# 3. DATASET AND DATALOADER\n",
        "# =========================================\n",
        "\n",
        "class TelemetryDataset(Dataset):\n",
        "    def __init__(self, X):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx]\n",
        "\n",
        "dataset = TelemetryDataset(X_real)\n",
        "\n",
        "batch_size = 256\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "input_dim = X_real.shape[1]\n",
        "latent_dim = 16\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# =========================================\n",
        "# 4. AUTOENCODER MODEL\n",
        "# =========================================\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, latent_dim),\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, input_dim),\n",
        "            nn.Sigmoid(),   # OK because inputs are scaled and one hot\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "        return x_hat\n",
        "\n",
        "model = Autoencoder(input_dim, latent_dim).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss(reduction=\"mean\")\n",
        "\n",
        "# =========================================\n",
        "# 5. MAKE TRAINING DP WITH OPACUS\n",
        "# =========================================\n",
        "\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "# sample_rate = batch_size / total_points is what DP uses\n",
        "sample_rate = batch_size / len(dataset)\n",
        "\n",
        "model, optimizer, dataloader = privacy_engine.make_private(\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=dataloader,\n",
        "    noise_multiplier=1.5,    # tune this\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "print(\"Sample rate:\", sample_rate)\n",
        "\n",
        "# =========================================\n",
        "# 6. TRAINING LOOP WITH DP SGD\n",
        "# =========================================\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        recon = model(batch)\n",
        "        loss = criterion(recon, batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item() * batch.size(0)\n",
        "\n",
        "    epoch_loss /= len(dataset)\n",
        "    eps = privacy_engine.accountant.get_epsilon(delta=1e-5)\n",
        "    print(f\"Epoch {epoch+1:02d} - Loss: {epoch_loss:.4f} - eps: {eps:.2f}\")\n",
        "\n",
        "# =========================================\n",
        "# 7. GENERATE SYNTHETIC DATA\n",
        "# =========================================\n",
        "\n",
        "model.eval()\n",
        "num_samples = len(df)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # You can also sample z from encoder outputs of real data\n",
        "    # For now, use standard normal\n",
        "    z = torch.randn(num_samples, latent_dim, device=device)\n",
        "    synthetic_matrix = model.decoder(z).cpu().numpy()\n",
        "\n",
        "# =========================================\n",
        "# 8. DECODE BACK TO ORIGINAL FEATURES\n",
        "# =========================================\n",
        "\n",
        "# 8a. Decode categorical from one hot\n",
        "cat_encoder = transformer.named_transformers_[\"cat\"]\n",
        "cat_sizes = [len(c) for c in cat_encoder.categories_]\n",
        "\n",
        "cat_end = sum(cat_sizes)\n",
        "cat_mat = synthetic_matrix[:, :cat_end]\n",
        "\n",
        "start = 0\n",
        "decoded_cats = []\n",
        "\n",
        "for i, size in enumerate(cat_sizes):\n",
        "    block = cat_mat[:, start:start + size]\n",
        "    labels = cat_encoder.categories_[i]\n",
        "    max_idx = block.argmax(axis=1)\n",
        "    decoded_cats.append(pd.Series(labels[max_idx], name=categorical_cols[i]))\n",
        "    start += size\n",
        "\n",
        "cat_df = pd.concat(decoded_cats, axis=1)\n",
        "\n",
        "# 8b. Decode numeric columns (still numeric seconds here)\n",
        "num_mat = synthetic_matrix[:, cat_end:]\n",
        "num_scaler = transformer.named_transformers_[\"num\"]\n",
        "num_real = num_scaler.inverse_transform(num_mat)\n",
        "\n",
        "num_df = pd.DataFrame(num_real, columns=numeric_cols)\n",
        "\n",
        "# Clip to real range to avoid crazy outliers\n",
        "real_min = df[\"TimeSeconds\"].min()\n",
        "real_max = df[\"TimeSeconds\"].max()\n",
        "num_df[\"TimeSeconds\"] = num_df[\"TimeSeconds\"].clip(real_min, real_max)\n",
        "\n",
        "# Create a human readable time column\n",
        "num_df[\"Time of Event\"] = pd.to_datetime(num_df[\"TimeSeconds\"], unit=\"s\")\n",
        "\n",
        "synthetic_df = pd.concat([cat_df, num_df], axis=1)\n",
        "\n",
        "synthetic_df.to_csv(\"data/synthetic_private_telemetry_data.csv\", index=False)\n",
        "print(\"Synthetic data saved to disk.\")\n",
        "\n",
        "# =========================================\n",
        "# 9. SIMPLE QUERY STYLE COMPARISONS\n",
        "# =========================================\n",
        "\n",
        "# For comparisons, keep using the numeric TimeSeconds,\n",
        "# not the datetime, so types match.\n",
        "\n",
        "def summarize_column(real, syn, col):\n",
        "    print(f\"\\n===== Summary for {col} =====\")\n",
        "    print(\"Real mean:   \", real[col].mean())\n",
        "    print(\"Synthetic mean:\", syn[col].mean())\n",
        "    print(\"Real median:   \", real[col].median())\n",
        "    print(\"Synthetic median:\", syn[col].median())\n",
        "    print(\"Real std:      \", real[col].std())\n",
        "    print(\"Synthetic std: \", syn[col].std())\n",
        "    print(\"Real min:      \", real[col].min())\n",
        "    print(\"Synthetic min: \", syn[col].min())\n",
        "    print(\"Real max:      \", real[col].max())\n",
        "    print(\"Synthetic max: \", syn[col].max())\n",
        "    print(\"Real IQR:      \", real[col].quantile(0.75) - real[col].quantile(0.25))\n",
        "    print(\"Synthetic IQR: \", syn[col].quantile(0.75) - syn[col].quantile(0.25))\n",
        "\n",
        "for col in numeric_cols:\n",
        "    summarize_column(df, synthetic_df, col)\n",
        "\n",
        "# Category distribution queries\n",
        "def compare_categorical(real_df, syn_df, col):\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    real_df[col].value_counts(normalize=True).sort_index().plot(kind=\"bar\", alpha=0.6, label=\"Real\")\n",
        "    syn_df[col].value_counts(normalize=True).sort_index().plot(kind=\"bar\", alpha=0.6, label=\"Synthetic\")\n",
        "    plt.title(\"Distribution: \" + col)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "for col in categorical_cols:\n",
        "    compare_categorical(df, synthetic_df, col)\n",
        "\n",
        "# Numeric distribution plots (no datetime here, so no error)\n",
        "def compare_numeric(real_df, syn_df, col):\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    plt.hist(real_df[col], bins=50, alpha=0.5, label=\"Real\")\n",
        "    plt.hist(syn_df[col], bins=50, alpha=0.5, label=\"Synthetic\")\n",
        "    plt.title(\"Histogram: \" + col)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "for col in numeric_cols:\n",
        "    compare_numeric(df, synthetic_df, col)\n",
        "\n",
        "# =========================================\n",
        "# 10. NEAREST NEIGHBOR PRIVACY TEST\n",
        "# =========================================\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Use the same feature space for both\n",
        "real_mat = X_real\n",
        "syn_mat = transformer.transform(synthetic_df[categorical_cols + numeric_cols])\n",
        "\n",
        "nbrs = NearestNeighbors(n_neighbors=1).fit(real_mat)\n",
        "distances, _ = nbrs.kneighbors(syn_mat)\n",
        "\n",
        "print(\"\\nNearest neighbor distances (first 10):\")\n",
        "print(np.sort(distances[:10], axis=0))\n",
        "print(\"Minimum distance:\", np.min(distances))\n",
        "print(\"Median distance:\", np.median(distances))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rt6BXbWB6Tx2"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "# 11. QUERY ENGINE\n",
        "# =========================================\n",
        "\n",
        "def query_mean(df, col):\n",
        "    return df[col].mean()\n",
        "\n",
        "def query_max(df, col):\n",
        "    return df[col].max()\n",
        "\n",
        "def query_min(df, col):\n",
        "    return df[col].min()\n",
        "\n",
        "def query_count(df, col, value):\n",
        "    \"\"\"How many times does a value appear in a column?\"\"\"\n",
        "    return (df[col] == value).sum()\n",
        "\n",
        "def query_value_counts(df, col):\n",
        "    \"\"\"Category distribution\"\"\"\n",
        "    return df[col].value_counts()\n",
        "\n",
        "def query_group_mean(df, group_col, value_col):\n",
        "    \"\"\"Mean of numeric column grouped by category\"\"\"\n",
        "    return df.groupby(group_col)[value_col].mean()\n",
        "\n",
        "def query_top_category(df, col):\n",
        "    \"\"\"Most common category\"\"\"\n",
        "    return df[col].value_counts().idxmax()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tM8DLPG6Waa",
        "outputId": "756e1efb-9b30-43e0-ef0e-e88413ba1240"
      },
      "outputs": [],
      "source": [
        "# Mean time\n",
        "query_mean(df, \"TimeSeconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uvltsVA60Dn",
        "outputId": "15d911af-945f-456c-eab9-4ae3d248d2be"
      },
      "outputs": [],
      "source": [
        "query_mean(synthetic_df, \"TimeSeconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAowo8FF6aI4",
        "outputId": "39a9b50b-ceb6-468c-948b-418c637a113d"
      },
      "outputs": [],
      "source": [
        "# Maximum time\n",
        "query_max(df, \"TimeSeconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WRNh64z6xSq",
        "outputId": "daf6046d-c9cc-4bec-d8c1-052be7f725d1"
      },
      "outputs": [],
      "source": [
        "query_max(synthetic_df, \"TimeSeconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4dfZcWQ6lqN",
        "outputId": "07fb905c-a682-497c-8029-ce12ab1e746b"
      },
      "outputs": [],
      "source": [
        "# Count\n",
        "query_count(df, \"Product Type\", \"Premium\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leXGlKWZ6ups",
        "outputId": "557c4ee5-4486-43a4-c0d6-48203ed608c4"
      },
      "outputs": [],
      "source": [
        "query_count(synthetic_df, \"Product Type\", \"Premium\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "JooAPPd96nJD",
        "outputId": "6006b197-3cdb-4b84-8dea-0cb689a231b1"
      },
      "outputs": [],
      "source": [
        "# Category Distribution\n",
        "query_value_counts(df, \"Product Type\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "aVhlSGun6rPu",
        "outputId": "326affaf-e19e-4f14-d543-0efb67358df5"
      },
      "outputs": [],
      "source": [
        "query_value_counts(synthetic_df, \"Product Type\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ITUXQ-mp7GUT",
        "outputId": "485dc02c-934d-42c9-db87-68a4a5d403a6"
      },
      "outputs": [],
      "source": [
        "# most common product type\n",
        "query_top_category(df, \"Product Type\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qs_-AF_M7K-t",
        "outputId": "1fe826e3-aa29-43c4-87d3-73736e2bf5ee"
      },
      "outputs": [],
      "source": [
        "query_top_category(synthetic_df, \"Product Type\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpyxxbDS4mu0",
        "outputId": "4db8c94a-7ecc-48dc-dc73-715295c3e504"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4yMxHTq4ZMx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
