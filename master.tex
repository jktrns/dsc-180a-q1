\documentclass[12pt,letterpaper]{article}
\usepackage{style/dsc180reportstyle}

\title{A comparative study of DP-SGD and Private Evolution for differentially private synthetic data}

\author{Mehak Kapur \\
  {\tt mekapur@ucsd.edu} \\\And
  Hana Tjendrawasi  \\
  {\tt htjendrawasi@ucsd.edu} \\\And
  Jason Tran \\
  {\tt jat037@ucsd.edu} \\\And
  Phuc Tran \\
  {\tt pct001@ucsd.edu} \\\And
  Yu-Xiang Wang \\
  {\tt yuxiangw@ucsd.edu} \\}

\begin{document}
\maketitle

\begin{abstract}
    Differentially private synthetic data generation enables the release of realistic datasets while rigorously protecting the privacy of individuals in the source data. Two methodologies dominate contemporary research: (1) differentially private stochastic gradient descent (DP-SGD), which privatizes model training through carefully calibrated noise injection, and (2) Private Evolution (PE), which achieves privacy guarantees through inference-only access to pre-trained foundation models. We implement both approaches on realistic datasets, evaluating their privacy-utility trade-offs under varying $\varepsilon$-budgets and computational constraints. Our objective is to assess whether the PE paradigm can rival the fidelity of DP-SGD while obviating the latter's computational and implementation complexity.
\end{abstract}

\begin{center}
Code: \url{https://github.com/jktrns/dsc-180a-q1}
\end{center}

\maketoc
\clearpage

\section{Introduction}

\subsection{Motivation and problem statement}

Contemporary machine learning requires large, richly annotated datasets that frequently encode sensitive personal information. The utility of such data for scientific and commercial progress stands in tension with the imperative to protect individual privacy. Traditional anonymization techniques (e.g., removal of explicit identifiers, aggregation) have proven inadequate against adversaries with auxiliary information. \textit{Differential privacy} (DP) provides a rigorous mathematical framework for this challenge, ensuring that any individual's participation in a dataset has negligible impact on the algorithm's output distribution.

The promise of differential privacy is quantified by a \textit{privacy budget} $\varepsilon$, where smaller values confer stronger privacy guarantees at the cost of reduced data utility. Thus, we have a trade-off that pervades all differentially private mechanisms; perfect privacy (corresponding to $\varepsilon = 0$) is achieved only by revealing nothing whatsoever, yet unbounded $\varepsilon$ permits arbitrary disclosure. We must operate within a regime of small but non-trivial $\varepsilon$, extracting maximal utility while preserving meaningful privacy.

Synthetic data generation under differential privacy offers a particularly appealing solution. In this setting, the curator (a trusted party) releases an entirely artificial dataset that captures the statistical properties of the original data while satisfying formal privacy guarantees. Such synthetic datasets come with the benefit of being analyzable using conventional statistical software. Moreover, once released, synthetic data imposes no ongoing privacy cost, unlike interactive mechanisms where privacy loss accumulates with each query.

Two methodologies have emerged for differentially private synthetic data generation:
\begin{enumerate}
    \item[(1)] \textbf{DP-SGD} \citep{abadi2016deep}: This method modifies the training procedure of generative models through per-sample gradient clipping and Gaussian noise addition. While effective, DP-SGD demands substantial computational power and full access to model internals.
    \item[(2)] \textbf{Private Evolution} \citep{lin2025differentiallyprivatesyntheticdata}: This approach eschews training entirely, instead achieving privacy through careful API calls to pre-trained foundation models.
\end{enumerate}
This naturally motivates the question: can training-free, low-complexity methods such as Private Evolution achieve utility comparable to that of DP-SGD?

We investigate both approaches empirically, comparing their practical trade-offs in privacy guarantees, computational efficiency, and output fidelity.

\subsection{Prior work}

Differential privacy, introduced by \citet{dwork2014algorithmic}, promises that the behavior of an algorithm is approximately invariant to the inclusion or exclusion of any single individual's data. This promise is formalized through the requirement that ``neighboring databases'' (those differing by precisely one record) yield indistinguishable output distributions, up to a multiplicative factor $e^\varepsilon$ and additive slack $\delta$. The foundational work established key mechanisms including the Laplace and Gaussian mechanisms for numeric queries, along with composition theorems demonstrating that privacy loss accumulates across multiple analyses.

The extension of differential privacy to deep learning proved non-trivial. \citet{abadi2016deep} developed DP-SGD by recognizing that standard gradient descent leaks information through the unbounded influence any single training example can exert on model parameters. Their solution bounds this influence through gradient clipping while injecting calibrated Gaussian noise to mask the contribution of individual samples. Critically, they introduced the moments accountant, a sophisticated privacy accounting technique that yields substantially tighter bounds on cumulative privacy loss than naive composition would suggest.

Building upon DP-SGD, \citet{ghalebikesabi2023differentially} demonstrated that fine-tuning diffusion models with differentially private optimization generates high-fidelity synthetic images. However, their DP-Diffusion approach requires substantial privacy budgets—on the order of $\varepsilon \approx 32$ for CIFAR-10—and demands considerable computational resources for training high-capacity generative models. These requirements limit practical deployment, particularly for resource-constrained organizations or highly sensitive datasets where large $\varepsilon$ values are inadmissible.

\citet{lin2025differentiallyprivatesyntheticdata} proposed a fundamentally different paradigm with their Private Evolution algorithm. Rather than training a model, PE operates exclusively through black-box API access to pre-trained foundation models. The algorithm iteratively selects and mutates synthetic candidates by constructing a differentially private histogram over nearest neighbors in an embedding space. This approach achieved Fréchet Inception Distance (FID, the metric for image generation quality) scores of $7.9$ or lower at $\varepsilon = 0.67$ on image generation tasks—substantially outperforming DP-Diffusion in both privacy and utility dimensions. The versatility of this approach has resulted in its extension in subsequent works to text \citep{xie2024differentiallyprivatesyntheticdata} and tabular data \citep{swanberg2025apiaccessllmsuseful}.

\section{Background}

\subsection{Differential privacy}

We represent databases as histograms $x \in \mathbb{N}^{|\mathcal{X}|}$ over a universe $\mathcal{X}$ of record types, where $x_i$ denotes the count of records of type $i \in \mathcal{X}$ (e.g., $\mathcal{X} = \{\text{student},\, \text{instructor},\, \text{staff}\}$, $x = (150, 30, 20)$). Two databases are \textit{neighboring} if they differ in the record count of exactly one individual (i.e., their $\ell_1$ distance is at most 1: $\|x - y\|_1 \leq 1$).

A randomized mechanism $\mathcal{M} \colon \mathbb{N}^{|\mathcal{X}|} \rightarrow \Delta(B)$ maps databases to probability distributions over outputs in range $B$, where $\Delta(B)$ denotes the probability simplex (i.e., the set of all probability distributions over $B$).

\begin{definition}[Differential Privacy \citep{dwork2014algorithmic}]
\label{def:dp}
$\mathcal{M}$ satisfies $(\varepsilon, \delta)$-differential privacy if for all neighboring databases $x, y$ with $\|x - y\|_1 \leq 1$ and all measurable sets $S \subseteq B$:
\begin{equation}
\Pr[\mathcal{M}(x) \in S] \leq e^{\varepsilon} \Pr[\mathcal{M}(y) \in S] + \delta
\end{equation}
When $\delta = 0$, we say $\mathcal{M}$ is $\varepsilon$-differentially private.
\end{definition}

The parameter $\varepsilon$ controls privacy: smaller values provide stronger guarantees but reduce utility. For small $\varepsilon$, the bound $e^\varepsilon \approx 1 + \varepsilon$ ensures outputs are nearly equally likely regardless of any individual's participation. The slack $\delta$ permits rare failures; we require $\delta \ll 1/\|x\|_1$.

\subsection{Basic mechanisms}

For numeric queries $f \colon \mathbb{N}^{|\mathcal{X}|} \rightarrow \mathbb{R}^k$, the \textit{$\ell_1$-sensitivity} is $\Delta f = \max_{\|x - y\|_1 = 1} \|f(x) - f(y)\|_1$, measuring the maximum change in $f$ when one record changes.

\textbf{Laplace mechanism.} Achieves $(\varepsilon, 0)$-DP by adding noise calibrated to sensitivity:
$$\mathcal{M}_L(x, f, \varepsilon) = f(x) + (Y_1, \ldots, Y_k), \quad Y_i \sim \text{Lap}(\Delta f / \varepsilon)$$
where $\text{Lap}(b)$ has density $p(y) = \frac{1}{2b}\exp(-|y|/b)$.

\textbf{Gaussian mechanism.} Achieves $(\varepsilon, \delta)$-DP by adding $Y_i \sim \mathcal{N}(0, \sigma^2)$ where $\sigma = \frac{\Delta_2 f}{\varepsilon} \sqrt{2 \ln(1.25/\delta)}$ and $\Delta_2 f$ is the $\ell_2$-sensitivity.

\subsection{Differentially private SGD}

DP-SGD \citep{abadi2016deep} privatizes neural network training through per-sample gradient clipping and noise injection. For each sample $i$ in mini-batch $B_t$:

\textbf{Clipping:} $\bar{g}_i = g_i \cdot \min(1, C/\|g_i\|_2)$ where $g_i = \nabla_\theta \ell(\theta; x_i)$

\textbf{Noising:} $\tilde{g}_t = \frac{1}{|B_t|} \sum_{i \in B_t} \bar{g}_i + \mathcal{N}(0, \sigma^2 C^2 I)$

The noise multiplier $\sigma$ controls privacy-utility tradeoff. For sampling rate $q = |B|/n$ over $T$ iterations, the moments accountant \citep{abadi2016deep} yields $(\varepsilon, \delta)$-DP with $\varepsilon = \mathcal{O}(q \sqrt{T \log(1/\delta)} / \sigma)$.

\textbf{Composition.} Privacy loss accumulates: $k$ mechanisms satisfying $(\varepsilon_i, \delta_i)$-DP yield $(\sum_i \varepsilon_i, \sum_i \delta_i)$-DP overall. Differential privacy is immune to post-processing: arbitrary data-independent transformations of $\mathcal{M}$'s output preserve the $(\varepsilon, \delta)$ guarantee.

\section{Experimental Methodology}

\subsection{Dataset and preprocessing}

We evaluate both DP-SGD and Private Evolution on a synthetic telemetry dataset designed to mimic privacy-sensitive operational logs. Although artificially generated, we treat this dataset as confidential to simulate realistic deployment scenarios where differential privacy mechanisms must operate under formal privacy constraints. The dataset comprises $n = 10{,}000$ event records spanning a three-month observation window (May 1–July 31, 2024), each characterized by attributes detailed in Table \ref{tab:data_schema}.

Let $x \in \mathbb{N}^{|\mathcal{X}|}$ denote the database histogram with $\|x\|_1 = 10{,}000$ records drawn from universe $\mathcal{X}$ defined by the Cartesian product of feature domains. Each record $r = (\text{product}, \text{event}, \text{timestamp}, \text{user}) \in \mathcal{X}$ encodes a discrete system event. The categorical features (product type, event type) exhibit typical operational distributions: product types span seven categories $\{\text{A}, \text{B}, \text{C}, \text{D}, \text{E}, \text{F}, \text{Other}\}$, while event types correspond to user and system actions $\{\text{open}, \text{close}, \text{save}, \text{reset}, \text{error}\}$. Timestamps are recorded at second-level granularity as Unix epoch values. User identifiers, though present in the raw data, are dropped prior to model training to prevent direct identification.

\textbf{Feature transformation.} We preprocess the raw dataset to obtain fixed-dimensional feature vectors amenable to gradient-based optimization. Categorical features undergo one-hot encoding, mapping each category to a binary indicator vector. The product type feature expands to $\mathbb{R}^7$ and event type to $\mathbb{R}^5$. Temporal features are standardized to zero mean and unit variance via $z = (t - \mu)/\sigma$, where $\mu$ and $\sigma$ denote the empirical mean and standard deviation of timestamps in the training data. This yields a composite feature vector $x \in \mathbb{R}^{13}$ for each record, with the first 12 dimensions encoding categorical memberships and the final dimension representing standardized time.

\begin{table}[h!]
\centering
\begin{tabular}{l p{9.5cm}}
\toprule
\textbf{Feature} & \textbf{Specification} \\
\midrule
Product Type & Categorical variable with domain $|\text{Product}| = 7$; one-hot encoded to binary vector $\{0,1\}^7$ where exactly one coordinate is unity \\ 
Event Type & Categorical variable with domain $|\text{Event}| = 5$; one-hot encoded to binary vector $\{0,1\}^5$ \\ 
Timestamp & Continuous temporal feature $t \in [t_{\min}, t_{\max}]$ measured in Unix epoch seconds; standardized via $z$-score transformation to facilitate gradient-based optimization \\
User ID & Categorical identifier with cardinality $|\text{Users}| \approx 1{,}000$; excised prior to model training to obviate direct re-identification risk \\
\bottomrule
\end{tabular}
\caption{Dataset schema and preprocessing pipeline. Feature transformations ensure compatibility with neural network architectures while preserving semantic structure necessary for utility evaluation.}
\label{tab:data_schema}
\end{table}

\subsection{Experimental protocols}

We conduct two classes of experiments, corresponding to distinct applications of differential privacy in data analysis.

\subsubsection{Differentially private statistical queries}

For the first set of experiments, we compute summary statistics $f \colon \mathbb{N}^{|\mathcal{X}|} \rightarrow \mathbb{R}^k$ using the Laplace mechanism. Queries of interest include:
\begin{itemize}
\item \textbf{Marginal histograms}: Frequency distributions of categorical features, e.g., $f_{\text{product}}(x) = (x_{\text{A}}, x_{\text{B}}, \ldots, x_{\text{Other}})$ with sensitivity $\Delta f = 1$
\item \textbf{Temporal statistics}: Mean timestamp, interquartile ranges, and event rate per unit time
\item \textbf{Contingency tables}: Joint distributions such as product-event co-occurrence matrices
\end{itemize}

For each query $f$ with $\ell_1$-sensitivity $\Delta f$, we release:
\begin{equation}
\mathcal{M}_L(x, f, \varepsilon) = f(x) + \text{Lap}(\Delta f / \varepsilon)^k
\end{equation}
We evaluate accuracy under varying privacy budgets $\varepsilon \in \{0.1, 0.5, 1.0, 5.0, 10.0\}$, measuring both absolute error $|f(x) - \mathcal{M}_L(x, f, \varepsilon)|$ and relative error normalized by the true query response.

\subsubsection{Differentially private synthetic data generation via DP-SGD}

For synthetic data generation, we implement DP-SGD to train an autoencoder that learns a compressed representation of the feature distribution. The architecture comprises:

\textbf{Encoder}: $\mathcal{E}_\phi \colon \mathbb{R}^{13} \rightarrow \mathbb{R}^{16}$, mapping feature vectors to a latent representation via:
\begin{align}
h_1 &= \text{ReLU}(W_1 x + b_1), \quad W_1 \in \mathbb{R}^{64 \times 13} \\
z &= W_2 h_1 + b_2, \quad W_2 \in \mathbb{R}^{16 \times 64}
\end{align}

\textbf{Decoder}: $\mathcal{D}_\theta \colon \mathbb{R}^{16} \rightarrow \mathbb{R}^{13}$, reconstructing feature vectors via:
\begin{align}
h_2 &= \text{ReLU}(W_3 z + b_3), \quad W_3 \in \mathbb{R}^{64 \times 16} \\
\hat{x} &= \sigma(W_4 h_2 + b_4), \quad W_4 \in \mathbb{R}^{13 \times 64}
\end{align}
where $\sigma(\cdot)$ denotes the sigmoid activation, appropriate given that one-hot encoded features are binary and standardized features are bounded.

The reconstruction objective minimizes mean squared error: $\mathcal{L}(\theta, \phi; x) = \|\mathcal{D}_\theta(\mathcal{E}_\phi(x)) - x\|_2^2$.

\textbf{Privacy enforcement.} We employ the Opacus library to modify the training procedure according to the DP-SGD protocol:
\begin{enumerate}
\item \textbf{Mini-batch sampling}: At each iteration $t$, sample a mini-batch $B_t$ of size $|B_t| = 256$ uniformly at random from the $n = 10{,}000$ training records (sampling rate $q = 0.0256$)
\item \textbf{Per-sample gradient clipping}: For each $x_i \in B_t$, compute $g_i = \nabla_{\theta,\phi} \mathcal{L}(\theta, \phi; x_i)$ and clip to $\ell_2$ norm $C = 1.0$
\item \textbf{Noise injection}: Aggregate clipped gradients and add Gaussian noise with multiplier $\sigma = 1.5$:
\begin{equation}
\tilde{g}_t = \frac{1}{|B_t|} \sum_{i \in B_t} \bar{g}_i + \mathcal{N}(0, (1.5 \cdot 1.0)^2 I)
\end{equation}
\item \textbf{Parameter update}: Apply standard Adam optimizer with learning rate $\alpha = 10^{-3}$
\end{enumerate}

We train for $T = 10$ epochs, tracking cumulative privacy loss via the moments accountant. With target privacy $\delta = 10^{-5}$ (satisfying $\delta \ll 1/n$), this configuration achieves $\varepsilon \approx 5$ upon training completion.

\textbf{Synthetic data generation.} Following training, we generate $n_{\text{syn}} = 10{,}000$ synthetic records by sampling latent codes $z \sim \mathcal{N}(0, I_{16})$ and decoding via $\tilde{x} = \mathcal{D}_\theta(z)$. The decoder outputs continuous values in $[0, 1]^{13}$, which we convert back to the original feature space:
\begin{itemize}
\item \textbf{Categorical features}: Extract the first 12 coordinates corresponding to one-hot encoded categories. For each categorical feature (product: 7 dimensions, event: 5 dimensions), apply $\argmax$ to recover the most probable category
\item \textbf{Temporal features}: Extract the final coordinate, invert the standardization via $t = z \cdot \sigma + \mu$, and clip to the observed temporal range $[t_{\min}, t_{\max}]$ to prevent extrapolation artifacts
\end{itemize}

\subsection{Evaluation metrics}

We assess synthetic data utility and privacy through complementary metrics addressing distributional fidelity and individual record protection.

\textbf{Distributional similarity.} We compare real and synthetic datasets through:
\begin{itemize}
\item \textbf{Marginal distributions}: For categorical features, compute $\chi^2$ divergence between empirical frequency tables. For temporal features, employ the Kolmogorov-Smirnov statistic measuring maximum discrepancy between cumulative distribution functions
\item \textbf{Summary statistics}: Compare means, medians, standard deviations, and interquartile ranges across all features
\item \textbf{Pairwise correlations}: Measure Pearson correlation between all feature pairs in both real and synthetic datasets, quantifying preservation of second-order dependencies
\end{itemize}

\textbf{Privacy validation.} While DP-SGD provides formal privacy guarantees, we empirically assess whether synthetic records inappropriately replicate real individuals through nearest-neighbor analysis:
\begin{equation}
d_{\min}(x_{\text{syn}}) = \min_{x_{\text{real}} \in X_{\text{real}}} \|x_{\text{syn}} - x_{\text{real}}\|_2
\end{equation}
We compute this minimum distance for all synthetic records in the transformed feature space. Synthetic records exhibiting $d_{\min} \approx 0$ would suggest near-memorization of real individuals, though such behavior should be precluded by the noise injection and privacy guarantees of DP-SGD. The distribution of minimum distances provides empirical evidence regarding whether synthetic data maintains appropriate separation from training records.

\section{Results}

[Your results section would go here]

\section{Discussion}

[Your discussion section would go here]

\section{Conclusion}

[Your conclusion would go here]

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}